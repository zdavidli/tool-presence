{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Notebook\n",
    "\n",
    "Based off MNIST VAE example from PyTorch https://github.com/pytorch/examples/tree/master/vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data used: Sinus Endoscopy Video from https://www.youtube.com/watch?v=6niL7Poc_qQ\n",
    "\n",
    "Simplifying Decisions:\n",
    "\n",
    "* Downscaling images to 28x28 so can train on personal machine\n",
    "* Using greyscale image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage import io\n",
    "from skimage.transform import rescale, resize\n",
    "\n",
    "# from src import endoscopy_dataset as eds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create a directory if not exists\n",
    "sample_dir = 'samples'\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "# image_size = 720 * 920\n",
    "image_size = 28*28\n",
    "h_dim = 400\n",
    "z_dim = 20\n",
    "num_epochs = 15\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder('data/surgical_video_frames/',\n",
    "                                           transform=transforms.Compose([\n",
    "                                                        transforms.Resize((28,28)),\n",
    "                                                        transforms.ToTensor(),\n",
    "                                           ]))\n",
    "\n",
    "data_loader = DataLoader(dataset=dataset,\n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the image here using matplotlib.\n",
    "def plot_image(tensor):\n",
    "    plt.figure()\n",
    "    # imshow needs a numpy array with the channel dimension\n",
    "    # as the the last dimension so we have to transpose things.\n",
    "    plt.imshow(tensor.numpy().transpose(1, 2, 0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(dataset[50][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=image_size, h_dim=h_dim, z_dim=z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(image_size, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc5 = nn.Linear(h_dim, image_size)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        return self.fc2(h), self.fc3(h)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        return torch.sigmoid(self.fc5(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconst = self.decode(z)\n",
    "        return x_reconst, mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidli/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/15], Step [10/13], Reconst Loss: 167976.7344, KL Div: 7480.9780\n",
      "Epoch[2/15], Step [10/13], Reconst Loss: 146095.3750, KL Div: 1159.6862\n",
      "Epoch[3/15], Step [10/13], Reconst Loss: 143577.9844, KL Div: 1047.9801\n",
      "Epoch[4/15], Step [10/13], Reconst Loss: 144157.9688, KL Div: 1731.9285\n",
      "Epoch[5/15], Step [10/13], Reconst Loss: 138030.0000, KL Div: 500.2059\n",
      "Epoch[6/15], Step [10/13], Reconst Loss: 134458.2344, KL Div: 737.2778\n",
      "Epoch[7/15], Step [10/13], Reconst Loss: 127429.2578, KL Div: 3524.3740\n",
      "Epoch[8/15], Step [10/13], Reconst Loss: 124559.5625, KL Div: 3964.4062\n",
      "Epoch[9/15], Step [10/13], Reconst Loss: 123176.7812, KL Div: 2021.1929\n",
      "Epoch[10/15], Step [10/13], Reconst Loss: 123531.0391, KL Div: 1650.9971\n",
      "Epoch[11/15], Step [10/13], Reconst Loss: 123247.1719, KL Div: 2768.7888\n",
      "Epoch[12/15], Step [10/13], Reconst Loss: 122140.7891, KL Div: 2475.5862\n",
      "Epoch[13/15], Step [10/13], Reconst Loss: 122420.3828, KL Div: 1615.9196\n",
      "Epoch[14/15], Step [10/13], Reconst Loss: 121359.1484, KL Div: 2459.9565\n",
      "Epoch[15/15], Step [10/13], Reconst Loss: 121362.2109, KL Div: 2706.6606\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (x, _) in enumerate(data_loader):\n",
    "        # Forward pass\n",
    "        x = x.to(device).view(-1, image_size)\n",
    "        x_reconst, mu, log_var = model(x)\n",
    "        \n",
    "        # Compute reconstruction loss and kl divergence\n",
    "        # For KL divergence, see Appendix B in VAE paper or http://yunjey47.tistory.com/43\n",
    "        reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=False)\n",
    "        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        loss = reconst_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\" \n",
    "                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item(), kl_div.item()))\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            # Save the sampled images\n",
    "            z = torch.randn(batch_size, z_dim).to(device)\n",
    "            out = model.decode(z).view(-1, 1, 28, 28)\n",
    "            save_image(out, os.path.join(sample_dir, 'sampled-{}.png'.format(epoch+1)))\n",
    "\n",
    "            # Save the reconstructed images\n",
    "            out, _, _ = model(x)\n",
    "            x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n",
    "            save_image(x_concat, os.path.join(sample_dir, 'reconst-{}.png'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
